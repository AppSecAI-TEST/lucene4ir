%!TEX root = lucene4IR2016workshop_report.tex
\subsection{Teaching and Learning}
\label{sec:teaching}


\noindent \todo{Juanma}

\noindent \todo{Krisztian}

\noindent \todo{Martin}


How do you go about teaching IR? What level?

What kinds of things do you need/want from such resources?

How do we see Lucene fitting in? Benefits to students?

\noindent \todo{Emanuele}

As part of the discussion the experience gained in several courses was reported.

One concerned with the course of {\em Information Systems (Advanced)} of the Master Degree in Statistical Science at the University of Padua.\footnote{The description refers to the course editions in the Academic Years 2011/12-2014/15. The teacher in charge was Massimo Melucci.}
The course covered both basic IR topics -- e.g. indexing and retrieval methods, retrieval models, and evaluation -- and more advanced topics such as Web Search or Machine Learning for IR; a detailed description of the course contents can be found in~\cite{Melucci2013}, an IR book that actually stemmed from the experience gained during the diverse course editions.
The course was designed so that, for most of the topics, lessons at a theoretical level on a specific topic were followed by a laboratory assignment on that topic.
The topics covered in laboratory assignments were: (i) creation of a test collection, (ii) indexing, (iii) retrieval, (iv) relevance feedback, (v) link analysis, (vi) Learning to Rank or Optimization of Ranking functions with parameters. Students were asked to propose their own methodology to carry out the laboratory activity: for instance, when considering the topic of ``relevance feedback'', each student could propose its own methodology to perform feedback, e.g. through a query expansion method or term re-weighting.
Each assignment involved the experimental evaluation on a shared test collection. Indeed, the objective of the assignments was twofold: first, to better understand the topic; second, to become familiar in the design and the implementation of experimental methodologies to evaluate methods and/or components and, more in general, to test research hypotheses.
Students were allowed to use a manual approach (when possible), a software library or build their own software modules to achieve the assignment objective; a list of software libraries were provided before the first laboratory assignment to make the students aware of possible options.
The adoption of a manual approach for some of the laboratory activities was mandatory. For instance, in the case of the assignment on indexing, the use of a manual approach aimed at a better understanding of the conceptual mechanisms to identify the most effective descriptors to retrieve relevant documents.
When the students proposed their own methodology for indexing, they were asked to present their approach as a set of steps that can be automated.
The availability of software libraries or resources to easily use the basic operations is crucial to allow the students to test their methodology with little effort; Apache Lucene is an example of such libraries if complemented with examples or resources.
In an edition of the course, a lesson was dedicated to a general introduction to Apache Lucene; sample code were also provided. Along with Apache Lucene also elasticsearch~\cite{elasticsearch} was presented, particularly how to index documents, perform retrieval, and how to customize the scoring mechanism via scripting.\footnote{Elasticsearch allows to evaluate a custom score via scripts --- see the {\em Scripting} module. Apache Solr provides similar functionalities via {\em Function Queries}.} The main reason for the introduction to elasticsearch was that the students could index, retrieve, and customize retrieval -- and therefore test some of their methodologies -- without writing actual code but only through the use of REST requests.
The course was attended not only by students in Statistical Science, but also from other degree courses such as Computer Science or Computer Engineering.
The heterogeneous background of the students was one of the reasons for not restricting the laboratory activities to a single software library and to allow also usage of libraries, e.g. elasticsearch, where interaction is possible at a ``higher'' level.
While students in Computer Science and Computer Engineering were familiar with Java, students in Statistical Science tended to prefer the R language because it was used in many courses within their course degree. Therefore, one of the resources that could be useful for teaching is a R wrapper for Apache Lucene --- wrappers in other programming languages exist, e.g. PyLucene~\cite{PyLucene} for Python.
Software libraries such as elasticsearch, could be useful tools to support teaching: for instance, they provide functionalities -- in the event of elasticsearch a REST request -- to display how a specific fragment of text is processed given a pipeline, e.g. a specific tokenizer and a set of filters (lowercase, porter stemming, $\dots$).


\begin{table}[]
  \small
\centering
\caption{Attempt at encoding the picture}
\label{my-label}
\begin{tabular}{|l|l|l|}
\hline
Apps                    & High Level                                                                                                            & Low Level                                                                                                                                                           \\ \hline
IndexerApp              & \begin{tabular}[c]{@{}l@{}}Modify how the indexer is performed\\ i.e. different tokenizers, parsers, etc\end{tabular} & Can modify parsers, tokenizers, etc                                                                                                                                 \\ \hline
IndexAnalyzerApp        & Inspect the influence of indexer                                                                                      &                                                                                                                                                                     \\ \hline
RetrievalApp            & \begin{tabular}[c]{@{}l@{}}Try out different retrieval algorithms\\ Change retrieval parameters\end{tabular}          & Implement new retrieval algorithms                                                                                                                                  \\ \hline
trec\_eval              & Measure the performance                                                                                               &                                                                                                                                                                     \\ \hline
ResultAnalyzerApp       & Inspect and analyze the results returned                                                                              & \begin{tabular}[c]{@{}l@{}}Customise the analysis, put out other \\ statistics of interest\end{tabular}                                                             \\ \hline
ExampleApp              &                                                                                                                       & \begin{tabular}[c]{@{}l@{}}Examples of how to work with the Lucene\\ index, to make modifications\end{tabular}                                                      \\ \hline
Batch Retrieval Scripts & \begin{tabular}[c]{@{}l@{}}Configure to run a series of standard\\ batch experiments\end{tabular}                     & \begin{tabular}[c]{@{}l@{}}Customize to run specific retrieval\\ experiments\end{tabular}                                                                           \\ \hline
RetrievalShellApp       & n/a                                                                                                                   & \begin{tabular}[c]{@{}l@{}}Customize to implement retrieval algorithms \\ outwith the Lucene scorer i.e. a simple scorer \\ assuming term independence\end{tabular} \\ \hline
\end{tabular}
\end{table}
